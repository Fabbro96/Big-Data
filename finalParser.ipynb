{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mwparserfromhell\n",
    "%pip install lxml\n",
    "%pip install pyspark\n",
    "%pip install nltk\n",
    "%pip install numba\n",
    "%pip install pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import os.path\n",
    "import shutil\n",
    "import mwparserfromhell\n",
    "from lxml import etree\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, lower, split, count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[1;32m   1349\u001b[0m               encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m   1350\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1282\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1328\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1328\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1277\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1037\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m \n\u001b[1;32m   1041\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 975\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1447\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mConnect to a host on a given (SSL) port.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1447\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:941\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mhttp.client.connect\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport)\n\u001b[0;32m--> 941\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection(\n\u001b[1;32m    942\u001b[0m     (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhost,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address)\n\u001b[1;32m    943\u001b[0m \u001b[39m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:824\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    823\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 824\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m getaddrinfo(host, port, \u001b[39m0\u001b[39;49m, SOCK_STREAM):\n\u001b[1;32m    825\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nltk\u001b[39m.\u001b[39;49mdownload(\u001b[39m\"\u001b[39;49m\u001b[39mstopwords\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m header \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mIndex\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/downloader.py:777\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow\u001b[39m(s, prefix2\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    769\u001b[0m     print_to(\n\u001b[1;32m    770\u001b[0m         textwrap\u001b[39m.\u001b[39mfill(\n\u001b[1;32m    771\u001b[0m             s,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    775\u001b[0m     )\n\u001b[0;32m--> 777\u001b[0m \u001b[39mfor\u001b[39;00m msg \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mincr_download(info_or_id, download_dir, force):\n\u001b[1;32m    778\u001b[0m     \u001b[39m# Error messages\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(msg, ErrorMessage):\n\u001b[1;32m    780\u001b[0m         show(msg\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/downloader.py:629\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39m# Look up the requested collection or package.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info_or_id(info_or_id)\n\u001b[1;32m    630\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    631\u001b[0m     \u001b[39myield\u001b[39;00m ErrorMessage(\u001b[39mNone\u001b[39;00m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError loading \u001b[39m\u001b[39m{\u001b[39;00minfo_or_id\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/downloader.py:603\u001b[0m, in \u001b[0;36mDownloader._info_or_id\u001b[0;34m(self, info_or_id)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_info_or_id\u001b[39m(\u001b[39mself\u001b[39m, info_or_id):\n\u001b[1;32m    602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(info_or_id, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 603\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo(info_or_id)\n\u001b[1;32m    604\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m         \u001b[39mreturn\u001b[39;00m info_or_id\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/downloader.py:1009\u001b[0m, in \u001b[0;36mDownloader.info\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minfo\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mid\u001b[39m):\n\u001b[1;32m   1007\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the ``Package`` or ``Collection`` record for the\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[39m    given item.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_index()\n\u001b[1;32m   1010\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_packages:\n\u001b[1;32m   1011\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_packages[\u001b[39mid\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/downloader.py:952\u001b[0m, in \u001b[0;36mDownloader._update_index\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url \u001b[39m=\u001b[39m url \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\n\u001b[1;32m    950\u001b[0m \u001b[39m# Download the index file.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39minternals\u001b[39m.\u001b[39mElementWrapper(\n\u001b[0;32m--> 952\u001b[0m     ElementTree\u001b[39m.\u001b[39mparse(urlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_url))\u001b[39m.\u001b[39mgetroot()\n\u001b[1;32m    953\u001b[0m )\n\u001b[1;32m    954\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_timestamp \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    956\u001b[0m \u001b[39m# Build a dictionary of packages.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[1;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[1;32m   1392\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m   1350\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "header = ['Index', 'Text']\n",
    "lista = []\n",
    "\n",
    "#Testando una la alive progress bar\n",
    "num_rows = 356900000\n",
    "counter = 0\n",
    "increment = 100000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic things"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Removing the sections titles and break if we are at the end of the part of the page we want to parse (some pages have a lot of sections that we don't want to parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNestedParentheses(s):\n",
    "    lines = s.split('\\n')\n",
    "    print(len(lines))\n",
    "    ret = []\n",
    "    for line in lines:\n",
    "        curr_line = ''\n",
    "        skip = 0\n",
    "        for i in line:\n",
    "            if i == '[':\n",
    "                skip += 1\n",
    "            elif i == ']'and skip > 0:\n",
    "                skip -= 1\n",
    "            elif skip == 0:\n",
    "                curr_line += i\n",
    "        ret.append(curr_line+\"\\n\")\n",
    "    return '\\n'.join(ret)\n",
    "\n",
    "text = '''\n",
    "I has a template! {{foo|bar|baz|eggs=spam}} See it <nowiki>or no<!-- revealed --></nowiki>[[File:wiki.png|thumb|Wikipedia logo]]? [[fox]] and [[dog|puppy]] New York also has [[public transport|public transportation]].\n",
    "\n",
    "\n",
    "New York is a state in the Northeastern United States and is the 27th-most extensive, fourth-most populous, and seventh-most densely populated U.S. state. New York is bordered by New Jersey and Pennsylvania to the south and Connecticut, Massachusetts, and Vermont to the east. The state has a maritime border in the Atlantic Ocean with Rhode Island, east of Long Island, as well as an international border with the Canadian provinces of Quebec to the north and Ontario to the west and north. The state of New York, with an estimated 19.8 million residents in 2015, is often referred to as New York State to distinguish it from New York City, the state's most populous city and its economic hub.\n",
    "With an estimated population of 8.55 million in 2015, New York City is the most populous city in the United States and the premier gateway for legal immigration to the United States. The New York City Metropolitan Area is one of the most populous urban agglomerations in the world. New York City is a global city, exerting a significant impact upon commerce, finance, media, art, fashion, research, technology, education, and entertainment, its fast pace defining the term New York minute. The home of the United Nations Headquarters, New York City is an important center for international diplomacy and has been described as the cultural and financial capital of the world, as well as the world's most economically powerful city. New York City makes up over 40% of the population of New York State. Two-thirds of the state's population lives in the New York City Metropolitan Area, and nearly 40% live on Long Island. Both the state and New York City were named for the 17th century Duke of York, future King James II of England. The next four most populous cities in the state are Buffalo, Rochester, Yonkers, and Syracuse, while the state capital is Albany.\n",
    "The earliest Europeans in New York were French colonists and Jesuit missionaries who arrived southward from settlements at Montreal for trade and proselytizing. New York had been inhabited by tribes of Algonquian and Iroquoian-speaking Native Americans for several hundred years by the time Dutch settlers moved into the region in the early 17th century. In 1609, the region was first claimed by Henry Hudson for the Dutch, who built Fort Nassau in 1614 at the confluence of the Hudson and Mohawk rivers, where the present-day capital of Albany later developed. The Dutch soon also settled New Amsterdam and parts of the Hudson Valley, establishing the colony of New Netherland, a multicultural community from its earliest days and a center of trade and immigration. The British annexed the colony from the Dutch in 1664. The borders of the British colony, the Province of New York, were similar to those of the present-day state.\n",
    "Many landmarks in New York are well known to both international and domestic visitors, with New York State hosting four of the world's ten most-visited tourist attractions in 2013: Times Square, Central Park, Niagara Falls (shared with Ontario), and Grand Central Terminal. New York is home to the Statue of Liberty, a symbol of the United States and its ideals of freedom, democracy, and opportunity. In the 21st century, New York has emerged as a global node of creativity and entrepreneurship, social tolerance, and environmental sustainability. New York's higher education network comprises approximately 200 colleges and universities, including Columbia University, Cornell University, New York University, and Rockefeller University, which have been ranked among the top 35 in the world.\n",
    "\n",
    "\n",
    "== History ==\n",
    "\n",
    "\n",
    "=== 16th century ===\n",
    "In 1524, Giovanni da Verrazzano, an Italian explorer in the service of the French crown, explored the Atlantic coast of North America between the Carolinas and Newfoundland, including New York Harbor and Narragansett Bay. On April 17, 1524 Verrazanno entered New York Bay, by way of the Strait now called the Narrows into the northern bay which he named Santa Margherita, in honour of the King of France's sister. Verrazzano described it as \"a vast coastline with a deep delta in which every kind of ship could pass\" and he adds: \"that it extends inland for a league and opens up to form a beautiful lake. This vast sheet of water swarmed with native boats\". He landed on the tip of Manhattan and perhaps on the furthest point of Long Island. Verrazanno's stay in this place was interrupted by a storm which pushed him north towards Martha's Vineyard.\n",
    "In 1540 French traders from New France built a chateau on Castle Island, within present-day Albany; due to flooding, it was abandoned the next year. In 1614, the Dutch under the command of Hendrick Corstiaensen, rebuilt the French chateau, which they called Fort Nassau. Fort Nassau was the first Dutch settlement in North America, and was located along the Hudson River, also within present-day Albany. The small fort served as a trading post and warehouse. Located on the Hudson River flood plain, the rudimentary \"fort\" was washed away by flooding in 1617, and abandoned for good after Fort Orange (New Netherland) was built nearby in 1623.\n",
    "\n",
    "\n",
    "=== 17th century ===\n",
    "\n",
    "Henry Hudson's 1609 voyage marked the beginning of European involvement with the area. Sailing for the Dutch East India Company and looking for a passage to Asia, he entered the Upper New York Bay on September 11 of that year. Word of his findings encouraged Dutch merchants to explore the coast in search for profitable fur trading with local Native American tribes.\n",
    "During the 17th century, Dutch trading posts established for the trade of pelts from the Lenape, Iroquois, and other tribes were founded in the colony of New Netherland. The first of these trading posts were Fort Nassau (1614, near present-day Albany); Fort Orange (1624, on the Hudson River just south of the current city of Albany and created to replace Fort Nassau), developing into settlement Beverwijck (1647), and into what became Albany; Fort Amsterdam (1625, to develop into the town New Amsterdam which is present-day New York City); and Esopus, (1653, now Kingston). The success of the patroonship of Rensselaerswyck (1630), which surrounded Albany and lasted until the mid-19th century, was also a key factor in the early success of the colony. The English captured the colony during the Second Anglo-Dutch War and governed it as the Province of New York. The city of New York was recaptured by the Dutch in 1673 during the Third Anglo-Dutch War (1672–1674) and renamed New Orange. It was returned to the English under the terms of the Treaty of Westminster a year later.\n",
    "\n",
    "\n",
    "== References ==\n",
    "\n",
    "\n",
    "== Further reading ==\n",
    "\n",
    "French, John Homer (1860). Historical and statistical gazetteer of New York State. Syracuse, New York: R. Pearsall Smith. OCLC 224691273. (Full text via Google Books.)\n",
    "New York State Historical Association (1940). New York: A Guide to the Empire State. New York City: Oxford University Press. ISBN 978-1-60354-031-5. OCLC 504264143. (Full text via Google Books.)\n",
    "\n",
    "\n",
    "== External links ==\n",
    "New York at DMOZ\n",
    " Geographic data related to New York at OpenStreetMap'''\n",
    "\n",
    "\n",
    "\n",
    "section_title_re = re.compile(\"^=+\\s+.*\\s+=+$\")\n",
    "content = ''\n",
    "skip = False\n",
    "\n",
    "for l in text.splitlines():\n",
    "    line = l.strip()\n",
    "    if \"= references =\" in line.lower():\n",
    "        skip = True  # replace with break if this is the last section\n",
    "        break\n",
    "    if \"= further reading =\" in line.lower():\n",
    "        skip = True  # replace with break if this is the last section\n",
    "        continue\n",
    "    if section_title_re.match(line):\n",
    "        skip = False\n",
    "        continue\n",
    "    if skip:\n",
    "        continue\n",
    "    content+=line+'\\n'\n",
    "    \n",
    "\"\"\" for x in content:\n",
    "    if x != \"\":\n",
    "        print(x.lower()+\"\\n\")\n",
    " \"\"\"\n",
    " \n",
    "withoutPipeLinks = (re.sub(r\"\\[\\[[^|\\]]*\\|([^|\\]]*)]]\", r\"\\1\", content, flags=re.S))\n",
    "withOutComments= re.sub('<!--.*?-->', '', withoutPipeLinks, flags=re.S)\n",
    "cleaned= re.sub('<.*?>', '', withOutComments, flags=re.S)\n",
    "testoPulito = removeNestedParentheses(cleaned)\n",
    "wikicode = mwparserfromhell.parse(testoPulito)\n",
    "templates = wikicode.filter_templates()\n",
    "text = \" \".join([testo for testo in testoPulito.split() if testo not in templates])\n",
    "print(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Idee per andare avanti:\n",
    "1. Eliminare prima le doppie graffe (di apertura e chiusura) eliminando il loro contenuto\n",
    "\n",
    "2. Capire come gestire il comportamento delle stringhe tipo `[[political movement|movement]]` dove l'importante è contenuto dopo la pipe.\n",
    "\n",
    "3. Eliminare qualsiasi segno di punteggiature (creando una lista di tutti i simboli di punteggiatura) andandoli a rimpiazzare con un singolo spazio in modo che ogni parola sia distanziata effettivamente dall'altra\n",
    "\n",
    "4. Effettuare un replace di tutti gli spazi (ce ne serve solo uno tra una parola e l'altra, non di più), basta usare la classe `re`\n",
    "\n",
    "5. Ora che abbiamo tutto l'occorrente, salvare su un file csv ed effettuare l'analisi con _pyspark_\n",
    "\n",
    "----\n",
    "_Domande al professore:_\n",
    "\n",
    "__Risposte da scrivere nella documentazione finale__\n",
    "\n",
    "1. I numeri sono visti come parole? Sì\n",
    "\n",
    "2. Teniamo conto dei titoli dei sotto-capitoli? No\n",
    "\n",
    "3. Teniamo conto delle ''references''? No\n",
    "\n",
    "4. Posso usare PonyORM?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNestedParentheses(s):\n",
    "    lines = s.split('\\n')\n",
    "    ret = []\n",
    "    for line in lines:\n",
    "        curr_line = ''\n",
    "        skip = 0\n",
    "        for i in line:\n",
    "            if i == '[' or i == '{' :\n",
    "                skip += 1\n",
    "            elif (i == ']' or i == '}' ) and skip > 0:\n",
    "                skip -= 1\n",
    "            elif skip == 0:\n",
    "                curr_line += i\n",
    "        ret.append(curr_line+\"\\n\")\n",
    "    return '\\n'.join(ret)\n",
    "\n",
    "def cleanUp(text):\n",
    "    section_title_re = re.compile(\"^=+\\s+.*\\s+=+$\")\n",
    "    content = ''\n",
    "    skip = False\n",
    "\n",
    "    for l in text.splitlines():\n",
    "        line = l.strip()\n",
    "        if \"= references =\" in line.lower():\n",
    "            skip = True  # replace with break if this is the last section\n",
    "            break\n",
    "        if \"= further reading =\" in line.lower():\n",
    "            skip = True  # replace with break if this is the last section\n",
    "            continue\n",
    "        if section_title_re.match(line):\n",
    "            skip = False\n",
    "            continue\n",
    "        if skip:\n",
    "            continue\n",
    "        content+=line+'\\n'\n",
    "        \n",
    "    \"\"\" for x in content:\n",
    "        if x != \"\":\n",
    "            print(x.lower()+\"\\n\")\n",
    "    \"\"\"\n",
    "    \n",
    "    withoutPipeLinks = (re.sub(r\"\\[\\[[^|\\]]*\\|([^|\\]]*)]]\", r\"\\1\", content, flags=re.S))\n",
    "    withOutComments= re.sub('<!--.*?-->', '', withoutPipeLinks, flags=re.S)\n",
    "    cleaned= re.sub('<.*?>', '', withOutComments, flags=re.S)\n",
    "    testoPulito = removeNestedParentheses(cleaned)\n",
    "    wikicode = mwparserfromhell.parse(testoPulito)\n",
    "    templates = wikicode.filter_templates()\n",
    "    text = \" \".join([testo for testo in testoPulito.split() if testo not in templates])\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    if text == \"\":\n",
    "        return \"NullTextFound\"\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulisco per bene il file tenendo soltanto le parole, pulendole da tutti i tag (parentesi quadrate, ad esempio) e da tutti i simboli di punteggiatura.\n",
    "Inoltre vado anche a cancellare i paragrafi ed i loro titoli. Da qui si creerá un metodo che verrá utilizzato "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vado a creare un file csv con i titoli della pagina e le parole in essa contenute. Vado inoltre ad eliminare tutte le stop words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Testing con un file piú piccolo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"file.csv\", \"w+\")\n",
    "writer = csv.writer(f)\n",
    "\n",
    "fileWords = open(\"fileWords.csv\", \"w+\")\n",
    "fileTitles = open(\"fileTitles.csv\", \"w+\")\n",
    "\n",
    "wordsWriter = csv.writer(fileWords)\n",
    "titlesWriter = csv.writer(fileTitles)\n",
    "\n",
    "#writer.writerow(header)\n",
    "titlesWriter.writerow([\"index\", \"title\"])\n",
    "wordsWriter.writerow([\"index\", \"text\"])\n",
    "section_title_re = re.compile(\"^=+\\s+.*\\s+=+$\")\n",
    "skip = False\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "totalCount = 0\n",
    "titles_list = []\n",
    "words_list = []\n",
    "file_xml= ''\n",
    "if os.path.isfile(\"/home/fabbro/Documents/Cartella/fileWiki.xml\"):\n",
    "    file_xml = \"/home/fabbro/Documents/Cartella/fileWiki.xml\"\n",
    "elif os.path.isfile(\"/home/fabbro/Documents/Big-Data/file.xml\"):\n",
    "    file_xml = \"/home/fabbro/Documents/Big-Data/file.xml\"\n",
    "else:\n",
    "    print(\"File non trovato\")\n",
    "tupla_titolo = ()\n",
    "tupla_testo = ()\n",
    "for event, elem in etree.iterparse(file_xml):\n",
    "    text = \"\"\n",
    "    title=\"\"\n",
    "    \n",
    "    if('title' in str(elem.tag)):\n",
    "        if(elem.text is not None):\n",
    "            i = i+1\n",
    "            title = elem.text.lower()\n",
    "            #titles_list.append(title)\n",
    "            #print(\"Count title: \"+str(i)+\" \"+title.lower())\n",
    "            titlesWriter.writerow([i, title])\n",
    "\n",
    "    if('text' in str(elem.tag)):\n",
    "        if(elem.text is not None):\n",
    "            j = j+1\n",
    "            text = elem.text.lower()\n",
    "            text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "            #print(\"Count text: \" + str(j) + \" \"+pulizia(text))\n",
    "            text = cleanUp(text)\n",
    "            #words_list.append(text)\n",
    "            wordsWriter.writerow([j, text])\n",
    "    if j == 1000000 and i == 1000000:\n",
    "        break\n",
    "    elem.clear()\n",
    "    totalCount += 1\n",
    "    if totalCount > 1 and (totalCount % 1000) == 0:\n",
    "            percents = (totalCount/1000000)*100\n",
    "            print(\"{:,}\".format(totalCount)+f\" rows written of 1,000,000 percents: {percents:.2f}%\", end=\"\\r\")\n",
    "            if percents == 100:\n",
    "                break\n",
    "#print(\"Are rows and titles equals? \"+str(len(words_list) == len(titles_list)))\n",
    "#print(\"Words ha la lunghezza di: \"+str(len(words_list))+\"mentre il contatore: \"+str(j))\n",
    "#print(\"Titles ha la lunghezza di: \"+str(len(titles_list))+\"mentre il contatore: \"+str(i))\n",
    "\n",
    "f.close()\n",
    "print(\"-------------------------------------------DONE------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"file.csv\", \"w+\")\n",
    "writer = csv.writer(f)\n",
    "\n",
    "fileWords = open(\"fileWords.csv\", \"w+\")\n",
    "fileTitles = open(\"fileTitles.csv\", \"w+\")\n",
    "\n",
    "wordsWriter = csv.writer(fileWords)\n",
    "titlesWriter = csv.writer(fileTitles)\n",
    "\n",
    "#writer.writerow(header)\n",
    "titlesWriter.writerow([\"index\", \"title\"])\n",
    "wordsWriter.writerow([\"index\", \"text\"])\n",
    "section_title_re = re.compile(\"^=+\\s+.*\\s+=+$\")\n",
    "skip = False\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "totalCount = 0\n",
    "titles_list = []\n",
    "words_list = []\n",
    "file_xml= ''\n",
    "if os.path.isfile(\"/home/fabbro/Documents/Cartella/fileWiki.xml\"):\n",
    "    file_xml = \"/home/fabbro/Documents/Cartella/fileWiki.xml\"\n",
    "elif os.path.isfile(\"/home/fabbro/Documents/Big-Data/file.xml\"):\n",
    "    file_xml = \"/home/fabbro/Documents/Big-Data/file.xml\"\n",
    "else:\n",
    "    print(\"File non trovato\")\n",
    "tupla_titolo = ()\n",
    "tupla_testo = ()\n",
    "for event, elem in etree.iterparse(file_xml):\n",
    "    text = \"\"\n",
    "    title=\"\"\n",
    "    \n",
    "    if('title' in str(elem.tag)):\n",
    "        if(elem.text is not None):\n",
    "            i = i+1\n",
    "            title = elem.text.lower()\n",
    "            #titles_list.append(title)\n",
    "            #print(\"Count title: \"+str(i)+\" \"+title.lower())\n",
    "            titlesWriter.writerow([i, title])\n",
    "\n",
    "    if('text' in str(elem.tag)):\n",
    "        if(elem.text is not None):\n",
    "            j = j+1\n",
    "            text = elem.text.lower()\n",
    "            text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "            #print(\"Count text: \" + str(j) + \" \"+pulizia(text))\n",
    "            text = cleanUp(text)\n",
    "            #words_list.append(text)\n",
    "            wordsWriter.writerow([j, text])\n",
    "    elem.clear()\n",
    "    totalCount += 1\n",
    "    if totalCount > 1 and (totalCount % 100000) == 0:\n",
    "            percents = (totalCount/356900000)*100\n",
    "            print(\"{:,}\".format(totalCount)+f\" rows written of 356,900,000, percents: {percents:.2f}%\", end=\"\\r\")\n",
    "#print(\"Are rows and titles equals? \"+str(len(words_list) == len(titles_list)))\n",
    "#print(\"Words ha la lunghezza di: \"+str(len(words_list))+\"mentre il contatore: \"+str(j))\n",
    "#print(\"Titles ha la lunghezza di: \"+str(len(titles_list))+\"mentre il contatore: \"+str(i))\n",
    "\n",
    "f.close()\n",
    "print(\"-------------------------------------------DONE------------------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inizio ad utilizzare PySpark utilizzando SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/06 11:59:27 WARN Utils: Your hostname, tayler resolves to a loopback address: 127.0.1.1; using 157.27.146.131 instead (on interface wlp0s20f3)\n",
      "23/04/06 11:59:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/06 11:59:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#spark = SparkSession.builder.appName(\"mySparkApp\").getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"mySparkApp\").config(\"spark.driver.memory\", \"10g\").config(\"spark.driver.maxResultSize\", \"10g\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Carico i due file come DF </b> <i> poi unendoli in un unico DF tenendo come chiave d'unione l'index</i>. Vado quindi a pulire quel che é stato generato (una cartella) mantenendo solo quel che ci serve a noi: il file csv con i dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"merged_file\"):\n",
    "    shutil.rmtree(\"merged_file\")\n",
    "\n",
    "titles_df = spark.read.csv(\"fileTitles.csv\", header=True, inferSchema=True)\n",
    "text_df = spark.read.csv(\"fileWords.csv\", header=True, inferSchema=True)\n",
    "\n",
    "merged_df = titles_df.join(text_df, \"index\")\n",
    "\n",
    "merged_df.coalesce(1).write.csv(\"merged_file\", header=True)\n",
    "\n",
    "for filename in os.listdir(\"merged_file\"):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.rename(os.path.join(\"merged_file\", filename), os.path.join(\"\", \"merged.csv\"))\n",
    "        shutil.rmtree(\"merged_file\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora bisogna mappare le parole (contando quante ce ne sono) presenti in ogni cella del file CSV. Stampo inoltre anche il titolo per far capire dove siamo arrivati."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo sembra una soluzione piú efficente di quella posta sopra. Molto piú efficente. Tiene conto quindi del titolo, la parola che é presente in quella pagina e quante volte viene utilizzata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, explode, split, regexp_replace\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"merged.csv\")\n",
    "\n",
    "df = df.select(\"title\", lower(\"text\").alias(\"text\"))\n",
    "\n",
    "words_df = df.select(\"title\", explode(split(regexp_replace(\"text\", r'\\W+', ' '), ' ')).alias(\"word\"))\n",
    "\n",
    "word_counts = words_df.groupBy(\"title\", \"word\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "word_counts.write.csv(\"countedWordsDir\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vado ad unire tutti i files CSV creati precedentemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# ottieni la lista di tutti i file CSV nella cartella\n",
    "csv_files = glob.glob(\"countedWordsDir/*.csv\")\n",
    "\n",
    "# crea una lista di dataframe leggendo i file CSV\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# unisci i dataframe in uno solo\n",
    "merged_df = pd.concat(dfs)\n",
    "\n",
    "# scrivi il dataframe unificato in un file CSV\n",
    "merged_df.to_csv(\"countedWords.csv\", index=False)\n",
    "\n",
    "shutil.rmtree(\"countedWordsDir\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>L’output deve contenere, per ogni parola, la lista di pagine di wikipedia\n",
    "che contengono quella parola</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quante volte sono state utilizzate le singole parole?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# leggi il file CSV di input\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"countedWords.csv\")\n",
    "\n",
    "# raggruppa le righe in base alla colonna \"word\" e somma la colonna \"count\"\n",
    "sum_df = df.groupBy(\"word\").agg(sum(\"count\").alias(\"total_count\"))\n",
    "sum_df = sum_df.withColumn(\"total_count\", sum_df[\"total_count\"].cast(\"int\"))\n",
    "\n",
    "# salva il risultato in un file CSV\n",
    "sum_df.write.mode(\"overwrite\").csv(\"paroleContate\", header=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unisco tutti i file .csv generati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# ottieni la lista di tutti i file CSV nella cartella\n",
    "csv_files = glob.glob(\"paroleContate/*.csv\")\n",
    "\n",
    "# crea una lista di dataframe leggendo i file CSV\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# unisci i dataframe in uno solo\n",
    "merged_df = pd.concat(dfs)\n",
    "\n",
    "# scrivi il dataframe unificato in un file CSV\n",
    "merged_df.to_csv(\"countedWordsFinalOutput.csv\", index=False)\n",
    "\n",
    "shutil.rmtree(\"paroleContate\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elimino la colonna che ci é inutile (la colonna count) salvando il risultato su un ultimo file finale, da analizzare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# leggi il file CSV\n",
    "df = pd.read_csv('countedWords.csv')\n",
    "\n",
    "# elimina la colonna\n",
    "df.drop('count', axis=1, inplace=True)\n",
    "\n",
    "# salva il DataFrame modificato in un nuovo file CSV\n",
    "df.to_csv('finalFileToAnalyze.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leggiamo tutte le parole presenti nella colonna \"word\" del file CSV. Utilizziamo un set in quanto non vogliamo che ci siano parole ripetute. Poi lo trasformiamo in una lista per renderlo indicizzabile e la trascriviamo in un file csv in modo da non dover eseguire ogni volta questa cella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# leggi il file CSV\n",
    "df = pd.read_csv('finalFileToAnalyze.csv')\n",
    "\n",
    "# crea una lista di tutte le parole\n",
    "word_list = []\n",
    "for row in df['word']:\n",
    "    words = re.findall(r'\\w+', str(row).lower())\n",
    "    word_list.extend(words)\n",
    "\n",
    "# rimuovi eventuali parole ripetute\n",
    "word_list = list(set(word_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora controllo quali parole sono presenti nel file finale da controllare. Se le parole sono presenti, aggiungo il titolo ad una lista. Si creerá infine una riga in un file CSV con la parola e la lista di titoli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 43610)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/fabbro/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/fabbro/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/fabbro/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/fabbro/.local/lib/python3.10/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Apri il file CSV contenente i titoli e le parole\n",
    "with open('finalFileToAnalyze.csv', newline='') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "\n",
    "    # Crea un dizionario per mantenere le associazioni tra parole e titoli\n",
    "    word_title_dict = {}\n",
    "\n",
    "    # Itera su ogni riga del file CSV\n",
    "    for row in reader:\n",
    "        # Estrai il titolo e la parola dalla riga\n",
    "        title = row['title']\n",
    "        word = row['word']\n",
    "\n",
    "        # Se la parola non è ancora presente nel dizionario, aggiungila\n",
    "        if word not in word_title_dict:\n",
    "            word_title_dict[word] = []\n",
    "\n",
    "        # Aggiungi il titolo alla lista di titoli associata alla parola\n",
    "        word_title_dict[word].append(title)\n",
    "\n",
    "# Apri un nuovo file CSV per scrivere i risultati\n",
    "with open('risultati.csv', mode='w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    # Scrivi l'intestazione delle colonne\n",
    "    writer.writerow(['word', 'titles'])\n",
    "\n",
    "    # Itera su ogni parola del tuo elenco di parole\n",
    "    for word in word_list:\n",
    "        # Se la parola è presente nel dizionario, estrai i titoli associati\n",
    "        if word in word_title_dict:\n",
    "            titles = word_title_dict[word]\n",
    "        else:\n",
    "            titles = []\n",
    "\n",
    "        # Scrivi una riga nel file CSV contenente la parola e i titoli associati\n",
    "        writer.writerow([word, titles])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
