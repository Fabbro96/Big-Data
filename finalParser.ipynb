{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mwparserfromhell in /home/fabbro/.local/lib/python3.10/site-packages (0.6.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lxml in /home/fabbro/.local/lib/python3.10/site-packages (4.9.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/fabbro/.local/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/fabbro/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/fabbro/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /home/fabbro/.local/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /home/fabbro/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /home/fabbro/.local/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/fabbro/.local/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numba in /home/fabbro/.local/lib/python3.10/site-packages (0.56.4)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /home/fabbro/.local/lib/python3.10/site-packages (from numba) (0.39.1)\n",
      "Requirement already satisfied: numpy<1.24,>=1.18 in /home/fabbro/.local/lib/python3.10/site-packages (from numba) (1.23.5)\n",
      "Requirement already satisfied: setuptools in /home/fabbro/.local/lib/python3.10/site-packages (from numba) (65.6.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/fabbro/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/fabbro/.local/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/fabbro/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/fabbro/.local/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/fabbro/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/fabbro/.local/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/fabbro/.local/lib/python3.10/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/fabbro/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.10/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/fabbro/.local/lib/python3.10/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/fabbro/.local/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3.10/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/fabbro/.local/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/fabbro/.local/lib/python3.10/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/fabbro/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /home/fabbro/.local/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /home/fabbro/.local/lib/python3.10/site-packages (from scipy) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post1.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2935 sha256=5449391b96bed4025f0a0d361bec334eeb325f5c21f919338821969145cbb215\n",
      "  Stored in directory: /home/fabbro/.cache/pip/wheels/c5/88/35/cc8a1e198b27a1d3d9d3f9b30090b9d5531778847835472ec4\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mwparserfromhell\n",
    "%pip install lxml\n",
    "%pip install pyspark\n",
    "%pip install nltk\n",
    "%pip install numba\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install scipy\n",
    "%pip install sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os.path\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import glob\n",
    "import mwparserfromhell\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from lxml import etree\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower, explode, split, regexp_replace, sum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/fabbro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "header = ['Index', 'Text']\n",
    "lista = []\n",
    "\n",
    "#Testando una la alive progress bar\n",
    "num_rows = 356900000\n",
    "counter = 0\n",
    "increment = 100000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic things"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def removeNestedParentheses(s):\n",
    "    lines = s.split('\\n')\n",
    "    print(len(lines))\n",
    "    ret = []\n",
    "    for line in lines:\n",
    "        curr_line = ''\n",
    "        skip = 0\n",
    "        for i in line:\n",
    "            if i == '[':\n",
    "                skip += 1\n",
    "            elif i == ']'and skip > 0:\n",
    "                skip -= 1\n",
    "            elif skip == 0:\n",
    "                curr_line += i\n",
    "        ret.append(curr_line+\"\\n\")\n",
    "    return '\\n'.join(ret)\n",
    "\n",
    "text = '''\n",
    "I has a template! {{foo|bar|baz|eggs=spam}} See it <nowiki>or no<!-- revealed --></nowiki>[[File:wiki.png|thumb|Wikipedia logo]]? [[fox]] and [[dog|puppy]] New York also has [[public transport|public transportation]].\n",
    "\n",
    "\n",
    "New York is a state in the Northeastern United States and is the 27th-most extensive, fourth-most populous, and seventh-most densely populated U.S. state. New York is bordered by New Jersey and Pennsylvania to the south and Connecticut, Massachusetts, and Vermont to the east. The state has a maritime border in the Atlantic Ocean with Rhode Island, east of Long Island, as well as an international border with the Canadian provinces of Quebec to the north and Ontario to the west and north. The state of New York, with an estimated 19.8 million residents in 2015, is often referred to as New York State to distinguish it from New York City, the state's most populous city and its economic hub.\n",
    "With an estimated population of 8.55 million in 2015, New York City is the most populous city in the United States and the premier gateway for legal immigration to the United States. The New York City Metropolitan Area is one of the most populous urban agglomerations in the world. New York City is a global city, exerting a significant impact upon commerce, finance, media, art, fashion, research, technology, education, and entertainment, its fast pace defining the term New York minute. The home of the United Nations Headquarters, New York City is an important center for international diplomacy and has been described as the cultural and financial capital of the world, as well as the world's most economically powerful city. New York City makes up over 40% of the population of New York State. Two-thirds of the state's population lives in the New York City Metropolitan Area, and nearly 40% live on Long Island. Both the state and New York City were named for the 17th century Duke of York, future King James II of England. The next four most populous cities in the state are Buffalo, Rochester, Yonkers, and Syracuse, while the state capital is Albany.\n",
    "The earliest Europeans in New York were French colonists and Jesuit missionaries who arrived southward from settlements at Montreal for trade and proselytizing. New York had been inhabited by tribes of Algonquian and Iroquoian-speaking Native Americans for several hundred years by the time Dutch settlers moved into the region in the early 17th century. In 1609, the region was first claimed by Henry Hudson for the Dutch, who built Fort Nassau in 1614 at the confluence of the Hudson and Mohawk rivers, where the present-day capital of Albany later developed. The Dutch soon also settled New Amsterdam and parts of the Hudson Valley, establishing the colony of New Netherland, a multicultural community from its earliest days and a center of trade and immigration. The British annexed the colony from the Dutch in 1664. The borders of the British colony, the Province of New York, were similar to those of the present-day state.\n",
    "Many landmarks in New York are well known to both international and domestic visitors, with New York State hosting four of the world's ten most-visited tourist attractions in 2013: Times Square, Central Park, Niagara Falls (shared with Ontario), and Grand Central Terminal. New York is home to the Statue of Liberty, a symbol of the United States and its ideals of freedom, democracy, and opportunity. In the 21st century, New York has emerged as a global node of creativity and entrepreneurship, social tolerance, and environmental sustainability. New York's higher education network comprises approximately 200 colleges and universities, including Columbia University, Cornell University, New York University, and Rockefeller University, which have been ranked among the top 35 in the world.\n",
    "\n",
    "\n",
    "== History ==\n",
    "\n",
    "\n",
    "=== 16th century ===\n",
    "In 1524, Giovanni da Verrazzano, an Italian explorer in the service of the French crown, explored the Atlantic coast of North America between the Carolinas and Newfoundland, including New York Harbor and Narragansett Bay. On April 17, 1524 Verrazanno entered New York Bay, by way of the Strait now called the Narrows into the northern bay which he named Santa Margherita, in honour of the King of France's sister. Verrazzano described it as \"a vast coastline with a deep delta in which every kind of ship could pass\" and he adds: \"that it extends inland for a league and opens up to form a beautiful lake. This vast sheet of water swarmed with native boats\". He landed on the tip of Manhattan and perhaps on the furthest point of Long Island. Verrazanno's stay in this place was interrupted by a storm which pushed him north towards Martha's Vineyard.\n",
    "In 1540 French traders from New France built a chateau on Castle Island, within present-day Albany; due to flooding, it was abandoned the next year. In 1614, the Dutch under the command of Hendrick Corstiaensen, rebuilt the French chateau, which they called Fort Nassau. Fort Nassau was the first Dutch settlement in North America, and was located along the Hudson River, also within present-day Albany. The small fort served as a trading post and warehouse. Located on the Hudson River flood plain, the rudimentary \"fort\" was washed away by flooding in 1617, and abandoned for good after Fort Orange (New Netherland) was built nearby in 1623.\n",
    "\n",
    "\n",
    "=== 17th century ===\n",
    "\n",
    "Henry Hudson's 1609 voyage marked the beginning of European involvement with the area. Sailing for the Dutch East India Company and looking for a passage to Asia, he entered the Upper New York Bay on September 11 of that year. Word of his findings encouraged Dutch merchants to explore the coast in search for profitable fur trading with local Native American tribes.\n",
    "During the 17th century, Dutch trading posts established for the trade of pelts from the Lenape, Iroquois, and other tribes were founded in the colony of New Netherland. The first of these trading posts were Fort Nassau (1614, near present-day Albany); Fort Orange (1624, on the Hudson River just south of the current city of Albany and created to replace Fort Nassau), developing into settlement Beverwijck (1647), and into what became Albany; Fort Amsterdam (1625, to develop into the town New Amsterdam which is present-day New York City); and Esopus, (1653, now Kingston). The success of the patroonship of Rensselaerswyck (1630), which surrounded Albany and lasted until the mid-19th century, was also a key factor in the early success of the colony. The English captured the colony during the Second Anglo-Dutch War and governed it as the Province of New York. The city of New York was recaptured by the Dutch in 1673 during the Third Anglo-Dutch War (1672–1674) and renamed New Orange. It was returned to the English under the terms of the Treaty of Westminster a year later.\n",
    "\n",
    "\n",
    "== References ==\n",
    "\n",
    "\n",
    "== Further reading ==\n",
    "\n",
    "French, John Homer (1860). Historical and statistical gazetteer of New York State. Syracuse, New York: R. Pearsall Smith. OCLC 224691273. (Full text via Google Books.)\n",
    "New York State Historical Association (1940). New York: A Guide to the Empire State. New York City: Oxford University Press. ISBN 978-1-60354-031-5. OCLC 504264143. (Full text via Google Books.)\n",
    "\n",
    "\n",
    "== External links ==\n",
    "New York at DMOZ\n",
    " Geographic data related to New York at OpenStreetMap'''\n",
    "\n",
    "\n",
    "\n",
    "section_title_re = re.compile(\"^=+\\s+.*\\s+=+$\")\n",
    "content = ''\n",
    "skip = False\n",
    "\n",
    "for l in text.splitlines():\n",
    "    line = l.strip()\n",
    "    if \"= references =\" in line.lower():\n",
    "        skip = True  # replace with break if this is the last section\n",
    "        break\n",
    "    if \"= further reading =\" in line.lower():\n",
    "        skip = True  # replace with break if this is the last section\n",
    "        continue\n",
    "    if section_title_re.match(line):\n",
    "        skip = False\n",
    "        continue\n",
    "    if skip:\n",
    "        continue\n",
    "    content+=line+'\\n'\n",
    "    \n",
    "\"\"\" for x in content:\n",
    "    if x != \"\":\n",
    "        print(x.lower()+\"\\n\")\n",
    " \"\"\"\n",
    " \n",
    "withoutPipeLinks = (re.sub(r\"\\[\\[[^|\\]]*\\|([^|\\]]*)]]\", r\"\\1\", content, flags=re.S))\n",
    "withOutComments= re.sub('<!--.*?-->', '', withoutPipeLinks, flags=re.S)\n",
    "cleaned= re.sub('<.*?>', '', withOutComments, flags=re.S)\n",
    "testoPulito = removeNestedParentheses(cleaned)\n",
    "wikicode = mwparserfromhell.parse(testoPulito)\n",
    "templates = wikicode.filter_templates()\n",
    "text = \" \".join([testo for testo in testoPulito.split() if testo not in templates])\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulisco per bene il file tenendo soltanto le parole, pulendole da tutti i tag (parentesi quadrate, ad esempio) e da tutti i simboli di punteggiatura.\n",
    "Inoltre vado anche a cancellare i paragrafi ed i loro titoli. Da qui si creerá un metodo che verrá utilizzato "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNestedParentheses(s):\n",
    "    lines = s.split('\\n')\n",
    "    ret = []\n",
    "    for line in lines:\n",
    "        curr_line = ''\n",
    "        skip = 0\n",
    "        for i in line:\n",
    "            if i == '[' or i == '{' :\n",
    "                skip += 1\n",
    "            elif (i == ']' or i == '}' ) and skip > 0:\n",
    "                skip -= 1\n",
    "            elif skip == 0:\n",
    "                curr_line += i\n",
    "        ret.append(curr_line+\"\\n\")\n",
    "    return '\\n'.join(ret)\n",
    "\n",
    "def cleanUp(text):\n",
    "    section_title_re = re.compile(\"^=+\\s+.*\\s+=+$\")\n",
    "    content = ''\n",
    "    skip = False\n",
    "\n",
    "    for l in text.splitlines():\n",
    "        line = l.strip()\n",
    "        if \"= references =\" in line.lower():\n",
    "            skip = True  # replace with break if this is the last section\n",
    "            break\n",
    "        if \"= further reading =\" in line.lower():\n",
    "            skip = True  # replace with break if this is the last section\n",
    "            continue\n",
    "        if section_title_re.match(line):\n",
    "            skip = False\n",
    "            continue\n",
    "        if skip:\n",
    "            continue\n",
    "        content+=line+'\\n'\n",
    "    \n",
    "    withoutPipeLinks = (re.sub(r\"\\[\\[[^|\\]]*\\|([^|\\]]*)]]\", r\"\\1\", content, flags=re.S))\n",
    "    withOutComments= re.sub('<!--.*?-->', '', withoutPipeLinks, flags=re.S)\n",
    "    cleaned= re.sub('<.*?>', '', withOutComments, flags=re.S)\n",
    "    testoPulito = removeNestedParentheses(cleaned)\n",
    "    wikicode = mwparserfromhell.parse(testoPulito)\n",
    "    templates = wikicode.filter_templates()\n",
    "    text = \" \".join([testo for testo in testoPulito.split() if testo not in templates])\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    if text == \"\":\n",
    "        return \"NullTextFound\"\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vado a creare un file csv con i titoli della pagina e le parole in essa contenute. Vado inoltre ad eliminare tutte le stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------DONE------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fileWords = open(\"fileWords.csv\", \"w+\")\n",
    "fileTitles = open(\"fileTitles.csv\", \"w+\")\n",
    "\n",
    "wordsWriter = csv.writer(fileWords)\n",
    "titlesWriter = csv.writer(fileTitles)\n",
    "\n",
    "titlesWriter.writerow([\"index\", \"title\"])\n",
    "wordsWriter.writerow([\"index\", \"text\"])\n",
    "section_title_re = re.compile(\"^=+\\s+.*\\s+=+$\")\n",
    "skip = False\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "totalCount = 0\n",
    "titles_list = []\n",
    "words_list = []\n",
    "file_xml= ''\n",
    "if os.path.isfile(\"/home/fabbro/Documents/Cartella/fileWiki.xml\"):\n",
    "    file_xml = \"/home/fabbro/Documents/Cartella/fileWiki.xml\"\n",
    "elif os.path.isfile(\"/home/fabbro/Documents/Big-Data/file.xml\"):\n",
    "    file_xml = \"/home/fabbro/Documents/Big-Data/file.xml\"\n",
    "else:\n",
    "    print(\"File non trovato\")\n",
    "tupla_titolo = ()\n",
    "tupla_testo = ()\n",
    "for event, elem in etree.iterparse(file_xml):\n",
    "    text = \"\"\n",
    "    title=\"\"\n",
    "    \n",
    "    if('title' in str(elem.tag)):\n",
    "        if(elem.text is not None):\n",
    "            i = i+1\n",
    "            title = elem.text.lower()\n",
    "            titlesWriter.writerow([i, title])\n",
    "\n",
    "    if('text' in str(elem.tag)):\n",
    "        if(elem.text is not None):\n",
    "            j = j+1\n",
    "            text = elem.text.lower()\n",
    "            text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "            text = cleanUp(text)\n",
    "            wordsWriter.writerow([j, text])\n",
    "    if j == 1000000 and i == 1000000:\n",
    "        break\n",
    "    elem.clear()\n",
    "    totalCount += 1\n",
    "    if totalCount > 1 and (totalCount % 1000) == 0:\n",
    "            percents = (totalCount/1000000)*100\n",
    "            print(\"{:,}\".format(totalCount)+f\" rows written of 1,000,000 percents: {percents:.2f}%\", end=\"\\r\")\n",
    "            if percents == 100:\n",
    "                break\n",
    "\n",
    "#f.close()\n",
    "print(\"-------------------------------------------DONE------------------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inizio ad utilizzare PySpark utilizzando SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/07 12:34:40 WARN Utils: Your hostname, kerah resolves to a loopback address: 127.0.1.1; using 192.168.178.79 instead (on interface wlp38s0)\n",
      "23/04/07 12:34:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/07 12:34:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"mySparkApp\").config(\"spark.driver.memory\", \"4g\").config(\"spark.driver.maxResultSize\", \"12g\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Carico i due file come DF </b> <i> poi unendoli in un unico DF tenendo come chiave d'unione l'index</i>. Vado quindi a pulire quel che é stato generato (una cartella) mantenendo solo quel che ci serve a noi: il file csv con i dati.\n",
    "\\\n",
    "<i> Attenzione: se esiste ancora una cartella di nome \"merged_file\", essa viene cancellata per evitare conflitti o errori. </i>\n",
    "\\\n",
    "\\\n",
    "Alla fine vado anche a cancellare fileWords.csv e fileTitles.csv in quanto non ci serviraranno piú."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/07 12:35:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(\"merged_file\"):\n",
    "    shutil.rmtree(\"merged_file\")\n",
    "\n",
    "titles_df = spark.read.csv(\"fileTitles.csv\", header=True, inferSchema=True)\n",
    "text_df = spark.read.csv(\"fileWords.csv\", header=True, inferSchema=True)\n",
    "\n",
    "merged_df = titles_df.join(text_df, \"index\")\n",
    "\n",
    "merged_df.coalesce(1).write.csv(\"merged_file\", header=True)\n",
    "\n",
    "for filename in os.listdir(\"merged_file\"):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.rename(os.path.join(\"merged_file\", filename), os.path.join(\"\", \"merged.csv\"))\n",
    "        shutil.rmtree(\"merged_file\")\n",
    "os.remove(\"fileWords.csv\")\n",
    "os.remove(\"fileTitles.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora bisogna mappare le parole (contando quante ce ne sono) presenti in ogni cella del file CSV. Tiene conto quindi del titolo, la parola che é presente in quella pagina e quante volte viene utilizzata in quel titolo.\n",
    "\\\n",
    "Eliminiamo merged.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/07 12:35:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/07 12:35:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/07 12:35:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/07 12:35:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/07 12:35:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/07 12:35:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/07 12:35:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/07 12:35:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/07 12:35:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/07 12:35:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/07 12:35:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/07 12:35:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"merged.csv\")\n",
    "df = df.select(\"title\", lower(\"text\").alias(\"text\"))\n",
    "\n",
    "words_df = df.select(\"title\", explode(split(regexp_replace(\"text\", r'\\W+', ' '), ' ')).alias(\"word\"))\n",
    "\n",
    "word_counts = words_df.groupBy(\"title\", \"word\").count().orderBy(\"count\", ascending=False)\n",
    "word_counts.write.csv(\"countedWordsDir\", mode=\"overwrite\", header=True)\n",
    "\n",
    "os.remove(\"merged.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vado ad unire tutti i files CSV creati precedentemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ottieni la lista di tutti i file CSV nella cartella\n",
    "csv_files = glob.glob(\"countedWordsDir/*.csv\")\n",
    "\n",
    "# crea una lista di dataframe leggendo i file CSV\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# unisci i dataframe in uno solo\n",
    "merged_df = pd.concat(dfs)\n",
    "\n",
    "# scrivi il dataframe unificato in un file CSV\n",
    "merged_df.to_csv(\"countedWords.csv\", index=False)\n",
    "\n",
    "shutil.rmtree(\"countedWordsDir\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>L’output deve contenere, per ogni parola, la lista di pagine di wikipedia\n",
    "che contengono quella parola</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quante volte sono state utilizzate le singole parole? Salviamo il risultato, poi, nel file \"countedWordsFinalOutput.csv\", unendo i file precedentemente generati separatamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"countedWords.csv\")\n",
    "\n",
    "# raggruppa le righe in base alla colonna \"word\" e somma la colonna \"count\"\n",
    "sum_df = df.groupBy(\"word\").agg(sum(\"count\").alias(\"total_count\"))\n",
    "sum_df = sum_df.withColumn(\"total_count\", sum_df[\"total_count\"].cast(\"int\"))\n",
    "\n",
    "sum_df.write.mode(\"overwrite\").csv(\"paroleContate\", header=True)\n",
    "\n",
    "#Unione dei file\n",
    "csv_files = glob.glob(\"paroleContate/*.csv\")\n",
    "\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(dfs)\n",
    "merged_df.to_csv(\"countedWordsFinalOutput.csv\", index=False)\n",
    "\n",
    "shutil.rmtree(\"paroleContate\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elimino la colonna che ci é inutile (la colonna count) salvando il risultato su un ultimo file finale, da analizzare. Elimino poi il file countedWords.csv che non ci servirá a nulla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('countedWords.csv')\n",
    "df.drop('count', axis=1, inplace=True)\n",
    "df.to_csv('finalFileToAnalyze.csv', index=False)\n",
    "os.remove('countedWords.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leggiamo tutte le parole presenti nella colonna \"word\" del file CSV. Utilizziamo un set in quanto non vogliamo che ci siano parole ripetute. Poi lo trasformiamo in una lista per renderlo indicizzabile e la trascriviamo in un file csv in modo da non dover eseguire ogni volta questa cella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('finalFileToAnalyze.csv')\n",
    "word_list = []\n",
    "for row in df['word']:\n",
    "    words = re.findall(r'\\w+', str(row).lower())\n",
    "    word_list.extend(words)\n",
    "\n",
    "word_list = list(set(word_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora controllo quali parole sono presenti nel file finale da controllare. Se le parole sono presenti, aggiungo il titolo ad una lista. Si creerá infine una riga in un file CSV con la parola e la lista di titoli.\n",
    "\\\n",
    "Eliminiamo <b>finalFileToAnalyze.csv</b> in quanto inutile. Dovremmo utilizzare risults.csv per le analisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apri il file CSV contenente i titoli e le parole\n",
    "with open('finalFileToAnalyze.csv', newline='') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "\n",
    "    # Crea un dizionario per mantenere le associazioni tra parole e titoli\n",
    "    word_title_dict = {}\n",
    "\n",
    "    # Itera su ogni riga del file CSV\n",
    "    for row in reader:\n",
    "        # Estrai il titolo e la parola dalla riga\n",
    "        title = row['title']\n",
    "        word = row['word']\n",
    "\n",
    "        # Se la parola non è ancora presente nel dizionario, aggiungila\n",
    "        if word not in word_title_dict:\n",
    "            word_title_dict[word] = []\n",
    "\n",
    "        # Aggiungi il titolo alla lista di titoli associata alla parola\n",
    "        word_title_dict[word].append(title)\n",
    "\n",
    "# Apri un nuovo file CSV per scrivere i risultati\n",
    "with open('results.csv', mode='w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    # Scrivi l'intestazione delle colonne\n",
    "    writer.writerow(['word', 'titles'])\n",
    "\n",
    "    # Itera su ogni parola del tuo elenco di parole\n",
    "    for word in word_list:\n",
    "        # Se la parola è presente nel dizionario, estrai i titoli associati\n",
    "        if word in word_title_dict:\n",
    "            titles = word_title_dict[word]\n",
    "        else:\n",
    "            titles = []\n",
    "\n",
    "        # Scrivi una riga nel file CSV contenente la parola e i titoli associati\n",
    "        writer.writerow([word, titles])\n",
    "os.remove(\"finalFileToAnalyze.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Iniziamo con i grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m norm\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcluster\u001b[39;00m \u001b[39mimport\u001b[39;00m KMeans\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m pairwise_distances\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamples_generator\u001b[39;00m \u001b[39mimport\u001b[39;00m make_blobs\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
